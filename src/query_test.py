import duckdb
import pandas as pd
import random
import time
import json
import argparse
from datetime import datetime
import os
from ingestion_test import get_system_metrics

# --- æŸ¥è¯¢ç”Ÿæˆå™¨ï¼šåŸºäºæ—¶é—´çª—å£ï¼Œæœ‰é—®é¢˜å‘½ä¸­æ¬¡æ•°å¤ªå°‘ã€‚ã€‚ã€‚ ---
def random_time_filter(df):
    # df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'], errors='coerce')
    # valid_times = df['tpep_pickup_datetime'].dropna()
    # if valid_times.empty:
    #     return None
    # min_date = valid_times.min().normalize()
    # max_date = valid_times.max().normalize() - pd.Timedelta(days=1)
    # if min_date >= max_date:
    #     return None
    # d1 = min_date + pd.Timedelta(days=random.randint(0, (max_date - min_date).days))
    # d2 = d1 + pd.Timedelta(days=1)
    # return f"SELECT * FROM {{table}} WHERE tpep_pickup_datetime >= '{d1}' AND tpep_pickup_datetime < '{d2}'", "time_window"
    pass


def random_month_filter(df):
    try:
        df['tpep_pickup_datetime'] = pd.to_datetime(
            df['tpep_pickup_datetime'],
            format='%m/%d/%Y %I:%M:%S %p',
            errors='coerce'
        )
    except Exception:
        return None
    valid_times = df['tpep_pickup_datetime'].dropna()
    if valid_times.empty:
        return None
    # è·å–æ‰€æœ‰æœˆåˆçš„æ—¥æœŸï¼ˆå»é‡ï¼‰
    months = valid_times.dt.to_period('M').dropna().unique()
    if len(months) == 0:
        return None
    # éšæœºé€‰æ‹©ä¸€ä¸ªæœˆ
    chosen_month = random.choice(months)
    d1 = chosen_month.to_timestamp()
    d2 = (chosen_month + 1).to_timestamp()
    return f"SELECT * FROM {{table}} WHERE tpep_pickup_datetime >= '{d1}' AND tpep_pickup_datetime < '{d2}'", "month_window"

# --- æŸ¥è¯¢ç”Ÿæˆå™¨ï¼šTop-K åœ°ç‚¹ ---
def random_topk_location(df):
    return "SELECT pulocationid, COUNT(*) FROM {table} GROUP BY pulocationid ORDER BY COUNT(*) DESC LIMIT 10", "topk"

# --- æŸ¥è¯¢ç”Ÿæˆå™¨ï¼šè¿‡æ»¤ trip_distance å’Œ total_amount ---
def random_filter_range(df):
    d_min, d_max = df['trip_distance'].quantile([0.3, 0.9])
    a_min, a_max = df['total_amount'].quantile([0.3, 0.9])
    d = round(random.uniform(d_min, d_max), 2)
    a = round(random.uniform(a_min, a_max), 2)
    return f"SELECT * FROM {{table}} WHERE trip_distance > {d} AND total_amount > {a}", "filter_range"

# --- æŸ¥è¯¢ç”Ÿæˆå™¨ï¼šéšæœº group by æŸåˆ— ---
def random_groupby(df):
    col = random.choice(['payment_type', 'passenger_count'])
    return f"SELECT {col}, COUNT(*) FROM {{table}} GROUP BY {col}", "groupby"

# --- è°ƒç”¨å¤šä¸ªæŸ¥è¯¢ç”Ÿæˆå™¨ï¼Œè¿”å›éšæœºæŸ¥è¯¢åˆ—è¡¨ ---
def generate_random_queries(df, table):
    query_generators = [random_month_filter, random_groupby, random_topk_location, random_filter_range]
    queries = []
    for gen in query_generators:
        result = gen(df)
        if not result:
            continue
        sql, qtype = result
        queries.append({
            'sql': sql.format(table=table),
            'type': qtype
        })
    return queries

# --- æ‰§è¡ŒæŸ¥è¯¢ + ç³»ç»ŸçŠ¶æ€ç›‘æ§ ---
def run_query(con, sql):
    start = time.time()
    result = con.execute(sql).fetchall()
    end = time.time()
    sys_metrics = get_system_metrics()
    return {
        'row_count': len(result),
        'time_taken_seconds': round(end - start, 5),
        'cpu_percent': sys_metrics.get('cpu_percent', -1),
        'memory_percent': sys_metrics.get('memory_percent', -1),
        'memory_used_gb': sys_metrics.get('memory_used_gb', -1)
    }

# --- ä¸»æµ‹è¯•é€»è¾‘ï¼šå¾ªç¯æ‰§è¡ŒæŸ¥è¯¢ï¼Œå®æ—¶å†™å…¥æ—¥å¿— ---
def benchmark_queries(db_path, table_name, sample_csv, log_path, rounds, max_seconds=None):
    df = pd.read_csv(sample_csv, nrows=10000)
    con = duckdb.connect(db_path)
    os.makedirs(os.path.dirname(log_path), exist_ok=True)
    start_time = time.time()

    with open(log_path, 'a', encoding='utf-8') as f:
        for i in range(rounds):
            if max_seconds and (time.time() - start_time) > max_seconds:
                print(f"â±ï¸ å·²è¾¾åˆ°æœ€å¤§è¿è¡Œæ—¶é—´ {max_seconds} ç§’ï¼Œåœæ­¢æµ‹è¯•")
                break

            print(f"\nğŸ” ç¬¬ {i+1} è½®æŸ¥è¯¢")
            for q in generate_random_queries(df, table_name):
                print(f"â¡ï¸ æŸ¥è¯¢ç±»å‹ [{q['type']}]: {q['sql'][:80]}...")
                result = run_query(con, q['sql'])
                result.update({
                    'timestamp': datetime.now().isoformat(),
                    'query': q['sql'],
                    'query_type': q['type']
                })
                f.write(json.dumps(result) + '\n')
                f.flush()

    print(f"\nâœ… æ—¥å¿—å†™å…¥å®Œæˆï¼š{log_path}")

# --- CLI å…¥å£ ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="DuckDB æŸ¥è¯¢æ€§èƒ½æµ‹è¯•ï¼ˆå«èµ„æºç›‘æ§ï¼‰")
    parser.add_argument('--db', required=True, help='DuckDB æ•°æ®åº“è·¯å¾„')
    parser.add_argument('--table', required=True, help='ç›®æ ‡è¡¨å')
    parser.add_argument('--sample', required=True, help='æ ·æœ¬ CSV æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--log', required=True, help='è¾“å‡ºæ—¥å¿—è·¯å¾„ (.jsonl)')
    parser.add_argument('--rounds', type=int, default=2**31-1, help='æœ€å¤§æŸ¥è¯¢è½®æ•°')
    parser.add_argument('--max-seconds', type=int, default=None, help='æœ€å¤§è¿è¡Œæ—¶é—´ï¼ˆç§’ï¼‰')

    args = parser.parse_args()

    benchmark_queries(
        db_path=args.db,
        table_name=args.table,
        sample_csv=args.sample,
        log_path=args.log,
        rounds=args.rounds,
        max_seconds=args.max_seconds
    )
