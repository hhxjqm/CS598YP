import duckdb
import pandas as pd
import random
import time
import json
import argparse
from datetime import datetime
import os
from src.ingestion_test import get_system_metrics, get_system_metrics_docker

random.seed(22)

# --- å¤šåˆ— group byï¼šæŒ‰ payment_type å’Œ passenger_count èšåˆ ---
def groupby_payment_and_passenger(df):
    """
    æŒ‰ payment_type å’Œ passenger_count ä¸¤åˆ—è¿›è¡Œ group by ç»Ÿè®¡æ•°é‡
    """
    return (
        "SELECT payment_type, passenger_count, COUNT(*) "
        "FROM {table} GROUP BY payment_type, passenger_count",
        "multi_column_groupby"   # ç±»å‹ç»Ÿä¸€æ”¹ä¸º multi_column_groupby
    )

# --- æŸ¥è¯¢ top-k ä¸Šè½¦åœ°ç‚¹ ---
def random_topk_location(df):
    """
    ç»Ÿè®¡ pulocationid å‡ºç°æ¬¡æ•°æœ€å¤šçš„å‰10ä¸ªåœ°ç‚¹
    """
    return (
        "SELECT pulocationid, COUNT(*) "
        "FROM {table} GROUP BY pulocationid "
        "ORDER BY COUNT(*) DESC LIMIT 10",
        "aggregation_topk"   # ç±»å‹ç»Ÿä¸€æ”¹ä¸º topk_location
    )

# --- ç­›é€‰ trip_distance å’Œ total_amount èŒƒå›´ ---
def random_filter_range(df):
    d_min, d_max = df['trip_distance'].quantile([0.3, 0.9])
    a_min, a_max = df['total_amount'].quantile([0.3, 0.9])

    d = round(random.uniform(d_min, d_max), 2)
    a = round(random.uniform(a_min, a_max), 2)

    # â­ å…³é”®ï¼šæŠŠä¸¤åˆ—éƒ½ CAST æˆ DOUBLE
    return (
        f"SELECT * FROM {{table}} "
        f"WHERE CAST(trip_distance AS DOUBLE) > {d} "
        f"  AND CAST(total_amount  AS DOUBLE) > {a}",
        "filter_range"
    )

# --- éšæœºé€‰æ‹©ä¸€åˆ—è¿›è¡Œ group by ---
def random_groupby(df):
    """
    åœ¨ payment_type æˆ– passenger_count ä¸¤åˆ—ä¸­éšæœºé€‰æ‹©ä¸€åˆ—åš group by
    """
    col = random.choice(['payment_type', 'passenger_count'])
    return (
        f"SELECT {col}, COUNT(*) FROM {{table}} GROUP BY {col}",
        "single_column_groupby"  # ç±»å‹ç»Ÿä¸€æ”¹ä¸º single_column_groupby
    )

def window_row_number(df):
    """
    1. Basic Window Function: Row Number
        åœºæ™¯
            ç»™æ¯ä¸€æ¡å‡ºç§Ÿè½¦è®¢å•æ‰“ä¸€ä¸ªå”¯ä¸€è¡Œå·ï¼Œä¾¿äºåç»­å¤„ç†ã€‚
    """
    return (
        "SELECT *, ROW_NUMBER() OVER () AS row_num FROM {table}",
        "basic_window"
    )

def sorted_window(df):
    return (
        "SELECT *, "
        "ROW_NUMBER() OVER (ORDER BY CAST(trip_distance AS DOUBLE) DESC) AS distance_rank "
        "FROM {table}",
        "sorted_window"
    )

def quantiles_entire_dataset(df):
    """
    3. Quantiles over Entire Dataset
        åœºæ™¯
            è®¡ç®—æ‰€æœ‰ä¹˜å®¢æ”¯ä»˜æ€»é‡‘é¢çš„ä¸­ä½æ•°ã€90%åˆ†ä½æ•°ï¼Œç”¨æ¥äº†è§£æ¶ˆè´¹æ°´å¹³åˆ†å¸ƒã€‚
    """
    return (
        "SELECT "
        "quantile_cont(CAST(total_amount AS DOUBLE), 0.5) OVER () AS median_amount, "
        "quantile_cont(CAST(total_amount AS DOUBLE), 0.9) OVER () AS p90_amount "
        "FROM {table}",
        "quantiles_entire_dataset"
    )

def partition_by_window(df):
    """
    4. Partition by Window Function: Row Number within Payment Type
        åœºæ™¯
            åœ¨æ¯ç§æ”¯ä»˜æ–¹å¼ï¼ˆä¾‹å¦‚ç°é‡‘ã€ä¿¡ç”¨å¡ï¼‰å†…éƒ¨å¯¹è®¢å•æ’åºï¼Œåˆ†æä¸åŒæ”¯ä»˜æ–¹å¼ä¸‹çš„è®¢å•ç‰¹ç‚¹ã€‚
    """
    return (
        "SELECT *, "
        "ROW_NUMBER() OVER (PARTITION BY payment_type "
        "ORDER BY CAST(trip_distance AS DOUBLE) DESC) AS rank_within_payment "
        "FROM {table}",
        "partition_by_window"
    )

def lead_and_lag(df):
    """
    5. Lead and Lag Analysis
        åœºæ™¯
            æ¯”è¾ƒæ¯å•å‰åçš„ä¹˜å®¢æ•°é‡å˜åŒ–ï¼Œè§‚å¯Ÿé«˜å³°æœŸã€ä½è°·æœŸç‰¹ç‚¹ã€‚
    """
    return (
        "SELECT passenger_count, "
        "LEAD(passenger_count) OVER (ORDER BY tpep_pickup_datetime) AS next_passenger, "
        "LAG(passenger_count) OVER (ORDER BY tpep_pickup_datetime) AS prev_passenger "
        "FROM {table}",
        "lead_and_lag"
    )

def moving_averages(df):
    """
    6. Moving Average over 3 Rows
        åœºæ™¯
            å¹³æ»‘å¤„ç†è¿ç»­3å•çš„æ”¯ä»˜é‡‘é¢ï¼Œåˆ†æä¹˜å®¢æ¶ˆè´¹å˜åŒ–è¶‹åŠ¿
    """
    return (
        "SELECT tpep_pickup_datetime, "
        # â­ æŠŠ total_amount æ˜¾å¼è½¬ DOUBLEï¼Œé¿å… AVG(VARCHAR)
        "AVG(CAST(total_amount AS DOUBLE)) "
        "     OVER (ORDER BY tpep_pickup_datetime "
        "           ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) AS moving_avg_amount "
        "FROM {table}",
        "moving_averages"
    )

def rolling_sum(df):
    """
    7. Rolling Sum over 3 Rows
        åœºæ™¯
            è®¡ç®—è¿ç»­3å•çš„æ€»æ”¯ä»˜é‡‘é¢ï¼Œç”¨æ¥è§‚å¯Ÿæ”¶å…¥å˜åŒ–ã€‚
    """
    return (
        "SELECT tpep_pickup_datetime, "
        # â­ åŒç†ï¼šSUM é‡Œ CAST
        "SUM(CAST(total_amount AS DOUBLE)) "
        "     OVER (ORDER BY tpep_pickup_datetime "
        "           ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) AS rolling_sum_amount "
        "FROM {table}",
        "rolling_sum"
    )

def range_between(df):
    """
    8. Cumulative Sum with Range Between
        åœºæ™¯
            è®¡ç®—ä»å¼€å§‹åˆ°å½“å‰å•çš„ç´¯è®¡æ”¶å…¥ï¼Œç”¨äºç»˜åˆ¶å¸æœºçš„å·¥ä½œæ—¥æ€»æ”¶å…¥æ›²çº¿ã€‚
    """
    return (
        "SELECT tpep_pickup_datetime, "
        # â­ CASTï¼Œå†ä¹Ÿä¸ä¼š sum(VARCHAR)
        "SUM(CAST(total_amount AS DOUBLE)) "
        "     OVER (ORDER BY tpep_pickup_datetime "
        "           RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_income "
        "FROM {table}",
        "range_between"
    )

def quantiles_partition_by(df):
    """
    9. Quantiles Partitioned by Payment Type
        åœºæ™¯
            è®¡ç®—æ¯ç§æ”¯ä»˜æ–¹å¼ä¸‹çš„ä¸­ä½æ”¯ä»˜é‡‘é¢ï¼Œæ¯”è¾ƒç°é‡‘å’Œä¿¡ç”¨å¡ä¹˜å®¢çš„æ¶ˆè´¹ä¹ æƒ¯å·®å¼‚ã€‚
    """
    return (
        "SELECT payment_type, "
        # â­ quantile_cont ä¹Ÿè¦æ•°å€¼åˆ—
        "quantile_cont(CAST(total_amount AS DOUBLE), 0.5) "
        "     OVER (PARTITION BY payment_type) AS median_amount_within_payment "
        "FROM {table}",
        "quantiles_partition_by"
    )

def multi_column_complex_aggregation(df):
    """
    ğŸ”¥ å¤æ‚å¤šåˆ—èšåˆæŸ¥è¯¢
    åœºæ™¯ï¼š
        å¤§é‡åˆ†ç»„ç»´åº¦ä¸‹ï¼Œåšå¤šç§å¤æ‚èšåˆï¼Œä¸“é—¨ç”¨äºæ‰“çˆ†CPUå’Œå†…å­˜
    """
    return (
        "SELECT "
        "passenger_count, "
        "payment_type, "
        "PULocationID, "
        "DOLocationID, "
        "EXTRACT(year  FROM tpep_pickup_datetime)  AS pickup_year, "
        "EXTRACT(month FROM tpep_pickup_datetime)  AS pickup_month, "
        "COUNT(*)                                    AS trip_count, "
        "SUM(CAST(total_amount  AS DOUBLE))          AS total_revenue, "
        "AVG(CAST(trip_distance AS DOUBLE))          AS avg_distance, "
        "MAX(CAST(tip_amount    AS DOUBLE))          AS max_tip, "
        "MIN(CAST(fare_amount   AS DOUBLE))          AS min_fare "
        "FROM {table} "
        "GROUP BY passenger_count, payment_type, PULocationID, "
        "         DOLocationID, pickup_year, pickup_month",
        "multi_column_complex_aggregation"
    )

def random_point_lookup(df):
    """
    æ¨¡æ‹Ÿç‚¹æŸ¥ï¼šéšæœºé€‰æ‹©ä¸€ä¸ª PULocationID æŸ¥æ‰¾å¯¹åº”è®°å½•
    """
    loc_id = random.choice(df['PULocationID'].dropna().unique().tolist())
    return (
        f"SELECT * FROM {{table}} WHERE PULocationID = {loc_id} LIMIT 5",
        "point_lookup"
    )

def random_datetime_range(df):
    """
    éšæœºé€‰æ‹©ä¸€ä¸ªæ—¶é—´èŒƒå›´ï¼Œæ¨¡æ‹Ÿç”¨æˆ·æŸ¥æŸä¸€å°æ—¶çš„è®°å½•
    """
    times = pd.to_datetime(df['tpep_pickup_datetime'].dropna(), errors='coerce')
    times = times.dropna()
    if times.empty:
        return (
            f"SELECT * FROM {{table}} WHERE passenger_count = 1 LIMIT 10",
            "fallback_datetime_range"
        )
    start = random.choice(times)
    end = start + pd.Timedelta(hours=1)
    return (
        f"SELECT VendorID, trip_distance, total_amount "
        f"FROM {{table}} "
        f"WHERE CAST(tpep_pickup_datetime AS TIMESTAMP) BETWEEN '{start}' AND '{end}' "
        f"ORDER BY total_amount DESC LIMIT 10",
        "datetime_range"
    )


def random_multi_column_filter(df):
    """
    å¤šåˆ—è”åˆè¿‡æ»¤ï¼šæŸ¥ç‰¹å®šä¸Šä¸‹è½¦ç‚¹å’Œä¹˜å®¢æ•°
    """
    loc_id = random.choice(df['PULocationID'].dropna().unique().tolist())
    doloc_id = random.choice(df['DOLocationID'].dropna().unique().tolist())
    pax = random.choice(df['passenger_count'].dropna().unique().tolist())
    return (
        f"SELECT trip_distance, fare_amount, tip_amount "
        f"FROM {{table}} "
        f"WHERE PULocationID = {loc_id} AND DOLocationID = {doloc_id} "
        f"AND passenger_count = {pax} LIMIT 10",
        "multi_column_filter"
    )

def tip_amount_above_zero(df):
    """
    åˆ†ææœ‰å°è´¹çš„è®¢å•ï¼šæŒ‰ VendorID åˆ†ç»„è®¡ç®—å¹³å‡ tip
    """
    return (
        f"SELECT VendorID, AVG(tip_amount) "
        f"FROM {{table}} WHERE tip_amount > 0 GROUP BY VendorID",
        "nonzero_tip_groupby"
    )



def generate_random_queries(df, table):
    normal_query_generators = [
        random_groupby,
        random_topk_location,
        random_filter_range,
        groupby_payment_and_passenger,

        # åŠ å…¥æ–°çš„ realistic æŸ¥è¯¢å‡½æ•°ï¼Œåªåœ¨mixæ—¶å€™è¿è¡Œï¼Œç‹¬è‡ªè¿è¡Œqueryçš„æ—¶å€™è®°å¾—æ³¨é‡Šã€‚
        random_point_lookup,
        random_datetime_range,
        random_multi_column_filter,
        tip_amount_above_zero
    ]

    heavy_query_generators = [
        window_row_number,
        sorted_window,
        quantiles_entire_dataset,
        partition_by_window,
        lead_and_lag,
        moving_averages,
        rolling_sum,
        range_between,
        quantiles_partition_by,
        multi_column_complex_aggregation
    ]

    queries = []
    for i in range(10):  # æ¯10è½®
        # å…ˆç”Ÿæˆæ™®é€šæŸ¥è¯¢
        for gen in random.sample(normal_query_generators, k=min(5, len(normal_query_generators))):
            try:
                sql, qtype = gen(df)
                queries.append({
                    'sql': sql.format(table=table),
                    'type': qtype
                })
            except Exception as e:
                print(f"âš ï¸ å¿½ç•¥ç”ŸæˆæŸ¥è¯¢å¤±è´¥: {gen.__name__} -> {e}")

        # æ¯è½®æ·»åŠ ä¸€ä¸ª heavy æŸ¥è¯¢
        heavy_gen = random.choice(heavy_query_generators)
        try:
            sql, qtype = heavy_gen(df)
            queries.append({
                'sql': sql.format(table=table),
                'type': qtype
            })
        except Exception as e:
            print(f"âš ï¸ å¿½ç•¥ heavy æŸ¥è¯¢ç”Ÿæˆå¤±è´¥: {heavy_gen.__name__} -> {e}")

    return queries


def create_indexes_duckdb(con, table_name):
    """
    åœ¨DuckDBä¸­å»ºæ¨èç´¢å¼•
    """
    try:
        con.execute(f"CREATE INDEX IF NOT EXISTS idx_passenger_count ON {table_name} (passenger_count);")
        con.execute(f"CREATE INDEX IF NOT EXISTS idx_payment_type ON {table_name} (payment_type);")
        con.execute(f"CREATE INDEX IF NOT EXISTS idx_trip_distance ON {table_name} (trip_distance);")
        con.execute(f"CREATE INDEX IF NOT EXISTS idx_pulocationid ON {table_name} (pulocationid);")
        con.execute(f"CREATE INDEX IF NOT EXISTS idx_total_amount ON {table_name} (total_amount);")
        con.execute(f"CREATE INDEX IF NOT EXISTS idx_pickup_datetime ON {table_name} (tpep_pickup_datetime);")
        print("âœ… DuckDBç´¢å¼•åˆ›å»ºå®Œæˆ")
    except Exception as e:
        print(f"âš ï¸ åˆ›å»ºDuckDBç´¢å¼•å¤±è´¥: {e}")


# --- æ‰§è¡ŒæŸ¥è¯¢ + ç³»ç»ŸçŠ¶æ€ç›‘æ§ ---
import psutil
import os

def run_query(con, sql):
    p = psutil.Process(os.getpid())
    cpu_times_start = p.cpu_times()
    wall_time_start = time.time()

    result = con.execute(sql).fetchall()

    wall_time_end = time.time()
    cpu_times_end = p.cpu_times()

    user_diff = cpu_times_end.user - cpu_times_start.user
    system_diff = cpu_times_end.system - cpu_times_start.system
    total_wall_time = wall_time_end - wall_time_start

    # è·å–é€»è¾‘CPUæ ¸æ•°
    cpu_count = psutil.cpu_count(logical=True) or 1

    if total_wall_time > 0:
        raw_cpu_percent = 100 * (user_diff + system_diff) / total_wall_time
        normalized_cpu_percent = raw_cpu_percent / cpu_count  # â­ é™¤ä»¥æ ¸æ•°ï¼
    else:
        normalized_cpu_percent = 0.0

    sys_metrics = get_system_metrics_docker()

    return {
        'row_count': len(result),
        'time_taken_seconds': round(total_wall_time, 5),
        'cpu_percent': round(normalized_cpu_percent, 2),   # â­ æ³¨æ„è¿™é‡Œ
        'memory_percent': sys_metrics.get('memory_percent', -1),
        'memory_used_gb': sys_metrics.get('memory_used_gb', -1)
    }



def wait_for_table(con, table, timeout=30):
    import time
    for _ in range(timeout):
        try:
            # å¦‚æœè¡¨å­˜åœ¨ï¼Œå°±ç›´æ¥è¿”å›
            con.execute(f"SELECT 1 FROM {table} LIMIT 1")
            print(f"âœ… è¡¨ {table} å·²å‡†å¤‡å¥½")
            return
        except duckdb.CatalogException:
            print(f"â³ ç­‰å¾…è¡¨ {table} åˆ›å»ºä¸­...")
            time.sleep(1)
    raise TimeoutError(f"âŒ è¡¨ {table} åœ¨ {timeout} ç§’å†…æœªå‡ºç°")

# --- ä¸»æµ‹è¯•é€»è¾‘ï¼šå¾ªç¯æ‰§è¡ŒæŸ¥è¯¢ï¼Œå®æ—¶å†™å…¥æ—¥å¿— ---
def benchmark_queries(db_path, table_name, sample_csv, log_path, rounds, max_seconds=None):
    df = pd.read_csv(sample_csv, nrows=10000)
    print("Successfully loaded")
    con = duckdb.connect(db_path)
    #create_indexes_duckdb(con, table_name)
    os.makedirs(os.path.dirname(log_path), exist_ok=True)
    wait_for_table(con, table_name, timeout=60)
    start_time = time.time()

    with open(log_path, 'a', encoding='utf-8') as f:
        for i in range(rounds):
            if max_seconds and (time.time() - start_time) > max_seconds:
                print(f"â±ï¸ å·²è¾¾åˆ°æœ€å¤§è¿è¡Œæ—¶é—´ {max_seconds} ç§’ï¼Œåœæ­¢æµ‹è¯•")
                break

            print(f"\nğŸ” ç¬¬ {i+1} è½®æŸ¥è¯¢")
            for q in generate_random_queries(df, table_name):
                print(f"â¡ï¸ æŸ¥è¯¢ç±»å‹ [{q['type']}]: {q['sql'][:80]}...")
                result = run_query(con, q['sql'])
                result.update({
                    'timestamp': datetime.now().isoformat(),
                    'query': q['sql'],
                    'query_type': q['type']
                })
                f.write(json.dumps(result) + '\n')
                f.flush()

    print(f"\nâœ… æ—¥å¿—å†™å…¥å®Œæˆï¼š{log_path}")

# --- CLI å…¥å£ ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="DuckDB æŸ¥è¯¢æ€§èƒ½æµ‹è¯•ï¼ˆå«èµ„æºç›‘æ§ï¼‰")
    parser.add_argument('--db', required=True, help='DuckDB æ•°æ®åº“è·¯å¾„')
    parser.add_argument('--table', required=True, help='ç›®æ ‡è¡¨å')
    parser.add_argument('--sample', required=True, help='æ ·æœ¬ CSV æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--log', required=True, help='è¾“å‡ºæ—¥å¿—è·¯å¾„ (.jsonl)')
    parser.add_argument('--rounds', type=int, default=2**31-1, help='æœ€å¤§æŸ¥è¯¢è½®æ•°')
    parser.add_argument('--max-seconds', type=int, default=None, help='æœ€å¤§è¿è¡Œæ—¶é—´ï¼ˆç§’ï¼‰')

    args = parser.parse_args()

    benchmark_queries(
        db_path=args.db,
        table_name=args.table,
        sample_csv=args.sample,
        log_path=args.log,
        rounds=args.rounds,
        max_seconds=args.max_seconds
    )
